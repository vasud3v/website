name: 24/7 Continuous Video Scraper & Uploader

on:
  schedule:
    - cron: '0 */5 * * *'
  
  workflow_dispatch:
    inputs:
      max_videos:
        description: 'Max videos to process (0 = unlimited)'
        required: false
        default: '0'
        type: string
      workers:
        description: 'Number of parallel workers'
        required: false
        default: '32'
        type: string
  
  push:
    branches:
      - main
    paths:
      - '.github/workflows/continuous-scraper.yml'

env:
  PYTHON_VERSION: '3.10'
  MAX_RUNTIME_MINUTES: 290

jobs:
  scrape-download-enrich-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 300
    
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Setup Chrome
        uses: browser-actions/setup-chrome@v1
        with:
          chrome-version: stable
      
      - name: Install System Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg wget curl aria2
          echo "✅ System dependencies installed"
          aria2c --version
      
      - name: Install Python Dependencies
        timeout-minutes: 10
        run: |
          python -m pip install --upgrade pip
          pip install -r javmix/requirements.txt
          pip install -r javdatabase/requirements.txt
          pip install -r upload_pipeline/requirements.txt
          pip install seleniumbase undetected-chromedriver deep-translator internetarchive
          echo "✅ Python dependencies installed"
      
      - name: Configure Environment
        timeout-minutes: 5
        run: |
          echo "SEEKSTREAMING_API_KEY=${{ secrets.SEEKSTREAMING_API_KEY }}" >> upload_pipeline/.env
          echo "STREAMTAPE_USERNAME=${{ secrets.STREAMTAPE_USERNAME }}" >> upload_pipeline/.env
          echo "STREAMTAPE_PASSWORD=${{ secrets.STREAMTAPE_PASSWORD }}" >> upload_pipeline/.env
          echo "TURBOVIPLAY_EMAIL=${{ secrets.TURBOVIPLAY_EMAIL }}" >> upload_pipeline/.env
          echo "TURBOVIPLAY_USERNAME=${{ secrets.TURBOVIPLAY_USERNAME }}" >> upload_pipeline/.env
          echo "TURBOVIPLAY_PASSWORD=${{ secrets.TURBOVIPLAY_PASSWORD }}" >> upload_pipeline/.env
          echo "TURBOVIPLAY_API_KEY=${{ secrets.TURBOVIPLAY_API_KEY }}" >> upload_pipeline/.env
          echo "VIDOZA_EMAIL=${{ secrets.VIDOZA_EMAIL }}" >> upload_pipeline/.env
          echo "VIDOZA_PASSWORD=${{ secrets.VIDOZA_PASSWORD }}" >> upload_pipeline/.env
          echo "VIDOZA_API_KEY=${{ secrets.VIDOZA_API_KEY }}" >> upload_pipeline/.env
          echo "UPLOADY_EMAIL=${{ secrets.UPLOADY_EMAIL }}" >> upload_pipeline/.env
          echo "UPLOADY_USERNAME=${{ secrets.UPLOADY_USERNAME }}" >> upload_pipeline/.env
          echo "UPLOADY_API_KEY=${{ secrets.UPLOADY_API_KEY }}" >> upload_pipeline/.env
          echo "UPLOAD18_API_KEY=${{ secrets.UPLOAD18_API_KEY }}" >> upload_pipeline/.env
          echo "IA_ACCESS_KEY=${{ secrets.IA_ACCESS_KEY }}" >> upload_pipeline/.env
          echo "IA_SECRET_KEY=${{ secrets.IA_SECRET_KEY }}" >> upload_pipeline/.env
          export IA_ACCESS_KEY="${{ secrets.IA_ACCESS_KEY }}"
          export IA_SECRET_KEY="${{ secrets.IA_SECRET_KEY }}"
          mkdir -p database downloaded_files upload_pipeline/upload_results javmix/downloaded_files javdatabase/downloaded_files
      
      - name: Restore Previous State
        uses: actions/cache/restore@v3
        with:
          path: |
            database/
            sitemap_videos.json
            javmix/new_videos.json
          key: scraper-state-${{ github.run_number }}
          restore-keys: |
            scraper-state-
      
      - name: Run Continuous Workflow
        id: workflow
        run: |
          echo "Starting continuous workflow..."
          python .github/workflows/continuous_workflow.py --max-runtime ${{ env.MAX_RUNTIME_MINUTES }} --workers ${{ github.event.inputs.workers || '32' }} --max-videos ${{ github.event.inputs.max_videos || '0' }}
      
      - name: Save State
        if: always()
        uses: actions/cache/save@v3
        with:
          path: |
            database/
            sitemap_videos.json
            javmix/new_videos.json
          key: scraper-state-${{ github.run_number }}
      
      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: workflow-results-${{ github.run_number }}
          path: |
            database/
            upload_pipeline/upload_results/
            *.log
          retention-days: 7
      
      - name: Generate Summary
        if: always()
        run: |
          echo "## Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ -f "workflow_stats.json" ]; then
            echo "### Statistics" >> $GITHUB_STEP_SUMMARY
            echo "Check workflow_stats.json for details" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Next Run" >> $GITHUB_STEP_SUMMARY
          echo "Scheduled in 5 hours" >> $GITHUB_STEP_SUMMARY
